{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_dir = \"data/train_test_val\"\n",
    "\n",
    "DATASET = {\n",
    "    'train': pd.read_csv(dataset_dir + '/train.csv').reset_index(drop=True), \n",
    "    'test': pd.read_csv(dataset_dir + '/test.csv').reset_index(drop=True), \n",
    "    'val': pd.read_csv(dataset_dir + '/val.csv').reset_index(drop=True), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Murder',\n",
       " 'Homicide',\n",
       " 'Robbery',\n",
       " 'Physical Injuries',\n",
       " 'Rape',\n",
       " 'Theft',\n",
       " 'Carnapping',\n",
       " 'Others']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = [label for label in DATASET['train'].keys() if label not in ['ID', 'Text']]\n",
    "id2label = {idx:label for idx, label in enumerate(LABELS)}\n",
    "label2id = {label:idx for idx, label in enumerate(LABELS)}\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/syke/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/syke/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Parrot paraphraser\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_sentence(text):\n",
    "    paraphrases = parrot.augment(input_phrase=text)\n",
    "    if paraphrases:\n",
    "        return paraphrases[0]  # Return the first paraphrase\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(dataset, num_samples_needed, augment_func):\n",
    "    augmented_data = []\n",
    "    current_data_size = len(dataset)\n",
    "    \n",
    "    while len(augmented_data) < num_samples_needed:\n",
    "        for _, row in dataset.iterrows():\n",
    "            text = row['Text']\n",
    "            \n",
    "            # Apply the augmentation function (synonym replacement or paraphrasing)\n",
    "            augmented_text = augment_func(text)\n",
    "            \n",
    "            # Create a new row with the augmented text and the same label(s)\n",
    "            new_row = row.copy()\n",
    "            new_row['Text'] = augmented_text\n",
    "            augmented_data.append(new_row)\n",
    "            \n",
    "            # Stop once we've generated enough samples\n",
    "            if len(augmented_data) >= num_samples_needed:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(augmented_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(text, n=2):\n",
    "    words = text.split()\n",
    "    new_text = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_text = [synonym if word == random_word else word for word in new_text]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    return ' '.join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "dataset_dir = \"data/train_test_val\"\n",
    "\n",
    "# Load the datasets\n",
    "DATASET = {\n",
    "    'train': pd.read_csv(dataset_dir + '/train.csv').reset_index(drop=True), \n",
    "    'test': pd.read_csv(dataset_dir + '/test.csv').reset_index(drop=True), \n",
    "    'val': pd.read_csv(dataset_dir + '/val.csv').reset_index(drop=True), \n",
    "}\n",
    "\n",
    "# Set the number of new samples needed\n",
    "NUM_TRAIN = 600\n",
    "NUM_TEST = 300\n",
    "NUM_VAL = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training data...\n"
     ]
    }
   ],
   "source": [
    "# Augment the datasets\n",
    "print(\"Augmenting training data...\")\n",
    "augmented_train = augment_dataset(DATASET['train'], NUM_TRAIN, synonym_replacement)\n",
    "augmented_train_full = pd.concat([DATASET['train'], augmented_train], ignore_index=True)\n",
    "augmented_train_full.to_csv(dataset_dir + '/augmented_train.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Augmenting test data...\")\n",
    "augmented_test = augment_dataset(DATASET['test'], NUM_TEST, paraphrase_sentence)\n",
    "augmented_test_full = pd.concat([DATASET['test'], augmented_test], ignore_index=True)\n",
    "augmented_test_full.to_csv(dataset_dir + '/augmented_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting validation data...\n",
      "Augmentation complete and saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Augmenting validation data...\")\n",
    "augmented_val = augment_dataset(DATASET['val'], NUM_VAL, paraphrase_sentence)\n",
    "augmented_val_full = pd.concat([DATASET['val'], augmented_val], ignore_index=True)\n",
    "augmented_val_full.to_csv(dataset_dir + '/augmented_val.csv', index=False)\n",
    "\n",
    "# Combine original and augmented datasets\n",
    "\n",
    "# Save the augmented datasets\n",
    "\n",
    "print(\"Augmentation complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and saved the augmented test and validation datasets!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely evaluate the string representation of a tuple\n",
    "\n",
    "# Path to your augmented CSV files\n",
    "augmented_test_file = \"data/train_test_val/augmented_test.csv\"\n",
    "augmented_val_file = \"data/train_test_val/augmented_val.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "# augmented_test = pd.read_csv(augmented_test_file)\n",
    "augmented_val = pd.read_csv(augmented_val_file)\n",
    "\n",
    "# Function to extract only the text from the tuple\n",
    "def extract_text(text_with_score):\n",
    "    # Safely evaluate the tuple string to convert it into a Python tuple\n",
    "    try:\n",
    "        text_tuple = ast.literal_eval(text_with_score)\n",
    "        return text_tuple[0]  # Return only the text part\n",
    "    except (ValueError, SyntaxError):\n",
    "        # In case the value is not a tuple, just return it as is\n",
    "        return text_with_score\n",
    "\n",
    "# Apply the function to the 'Text' column (or the column containing the paraphrased text)\n",
    "augmented_test['Text'] = augmented_test['Text'].apply(extract_text)\n",
    "augmented_val['Text'] = augmented_val['Text'].apply(extract_text)\n",
    "\n",
    "# Save the cleaned CSV files back\n",
    "# augmented_test.to_csv(\"data/train_test_val/cleaned_augmented_test.csv\", index=False)\n",
    "augmented_val.to_csv(\"data/train_test_val/cleaned_augmented_val.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned and saved the augmented test and validation datasets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder: 344\n",
      "Homicide: 356\n",
      "Robbery: 411\n",
      "Physical Injuries: 351\n",
      "Rape: 385\n",
      "Theft: 316\n",
      "Carnapping: 347\n",
      "Others: 300\n"
     ]
    }
   ],
   "source": [
    "label_counts = {label: DATASET['train'][label].sum() for label in LABELS}\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test label count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder: 179\n",
      "Homicide: 179\n",
      "Robbery: 209\n",
      "Physical Injuries: 181\n",
      "Rape: 200\n",
      "Theft: 158\n",
      "Carnapping: 148\n",
      "Others: 153\n"
     ]
    }
   ],
   "source": [
    "label_counts = {label: DATASET['test'][label].sum() for label in LABELS}\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping texts between train and val: 134\n",
      "Number of overlapping texts between train and test: 55\n",
      "Number of overlapping texts between val and test: 28\n",
      "2400\n",
      "1200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "train_ids = set(DATASET['train']['Text'])\n",
    "val_ids = set(DATASET['test']['Text'])\n",
    "test_ids = set(DATASET['val']['Text'])\n",
    "\n",
    "train_val_overlap = train_ids.intersection(val_ids)\n",
    "train_test_overlap = train_ids.intersection(test_ids)\n",
    "val_test_overlap = val_ids.intersection(test_ids)\n",
    "\n",
    "print(f\"Number of overlapping texts between train and val: {len(train_val_overlap)}\")\n",
    "print(f\"Number of overlapping texts between train and test: {len(train_test_overlap)}\")\n",
    "print(f\"Number of overlapping texts between val and test: {len(val_test_overlap)}\")\n",
    "\n",
    "\n",
    "print(len(DATASET['train']['Text']))\n",
    "print(len(DATASET['test']['Text']))\n",
    "print(len(DATASET['val']['Text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/syke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/syke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words for label 'Murder':\n",
      " 1. saw: 154\n",
      " 2. man: 140\n",
      " 3. murder: 129\n",
      " 4. house: 110\n",
      " 5. shot: 110\n",
      " 6. like: 106\n",
      " 7. act: 90\n",
      " 8. heard: 86\n",
      " 9. one: 81\n",
      " 10. calculated: 79\n",
      " 11. planned: 77\n",
      " 12. started: 74\n",
      " 13. way: 74\n",
      " 14. coldblooded: 73\n",
      " 15. left: 72\n",
      " 16. deliberate: 61\n",
      " 17. guy: 60\n",
      " 18. clear: 59\n",
      " 19. found: 58\n",
      " 20. made: 58\n",
      "\n",
      "Most common words for label 'Homicide':\n",
      " 1. man: 269\n",
      " 2. saw: 172\n",
      " 3. heard: 135\n",
      " 4. head: 127\n",
      " 5. one: 125\n",
      " 6. hit: 116\n",
      " 7. started: 109\n",
      " 8. victim: 103\n",
      " 9. fell: 83\n",
      " 10. woman: 81\n",
      " 11. trying: 75\n",
      " 12. unintentional: 72\n",
      " 13. tried: 71\n",
      " 14. pulled: 70\n",
      " 15. loud: 68\n",
      " 16. like: 66\n",
      " 17. grabbed: 65\n",
      " 18. knife: 64\n",
      " 19. floor: 64\n",
      " 20. ran: 64\n",
      "\n",
      "Most common words for label 'Robbery':\n",
      " 1. man: 313\n",
      " 2. using: 127\n",
      " 3. leaving: 123\n",
      " 4. saw: 123\n",
      " 5. money: 105\n",
      " 6. car: 103\n",
      " 7. terrifying: 101\n",
      " 8. wallet: 97\n",
      " 9. valuables: 91\n",
      " 10. heard: 90\n",
      " 11. hand: 88\n",
      " 12. tactics: 81\n",
      " 13. left: 77\n",
      " 14. house: 76\n",
      " 15. gun: 74\n",
      " 16. employing: 73\n",
      " 17. one: 72\n",
      " 18. menacing: 71\n",
      " 19. grabbed: 71\n",
      " 20. threats: 70\n",
      "\n",
      "Most common words for label 'Physical Injuries':\n",
      " 1. man: 246\n",
      " 2. saw: 166\n",
      " 3. one: 134\n",
      " 4. ground: 120\n",
      " 5. pain: 119\n",
      " 6. injured: 116\n",
      " 7. face: 107\n",
      " 8. started: 106\n",
      " 9. victim: 105\n",
      " 10. tried: 101\n",
      " 11. suddenly: 94\n",
      " 12. heard: 87\n",
      " 13. away: 85\n",
      " 14. could: 81\n",
      " 15. car: 78\n",
      " 16. trying: 74\n",
      " 17. fell: 74\n",
      " 18. woman: 73\n",
      " 19. grabbed: 71\n",
      " 20. attacker: 71\n",
      "\n",
      "Most common words for label 'Rape':\n",
      " 1. started: 198\n",
      " 2. saw: 164\n",
      " 3. man: 151\n",
      " 4. tried: 148\n",
      " 5. away: 121\n",
      " 6. woman: 118\n",
      " 7. could: 114\n",
      " 8. felt: 109\n",
      " 9. trying: 98\n",
      " 10. grabbed: 96\n",
      " 11. like: 95\n",
      " 12. kept: 84\n",
      " 13. one: 83\n",
      " 14. stop: 79\n",
      " 15. grope: 76\n",
      " 16. help: 70\n",
      " 17. night: 68\n",
      " 18. room: 67\n",
      " 19. back: 67\n",
      " 20. push: 66\n",
      "\n",
      "Most common words for label 'Theft':\n",
      " 1. bag: 154\n",
      " 2. someone: 144\n",
      " 3. wallet: 119\n",
      " 4. man: 111\n",
      " 5. phone: 98\n",
      " 6. pocket: 86\n",
      " 7. unnoticeably: 79\n",
      " 8. without: 72\n",
      " 9. gone: 72\n",
      " 10. woman: 67\n",
      " 11. crowd: 63\n",
      " 12. covertly: 61\n",
      " 13. noticed: 57\n",
      " 14. crowded: 57\n",
      " 15. quietly: 56\n",
      " 16. thief: 56\n",
      " 17. later: 54\n",
      " 18. saw: 54\n",
      " 19. left: 53\n",
      " 20. realized: 51\n",
      "\n",
      "Most common words for label 'Carnapping':\n",
      " 1. car: 709\n",
      " 2. man: 161\n",
      " 3. lot: 152\n",
      " 4. parked: 142\n",
      " 5. vehicle: 131\n",
      " 6. drove: 106\n",
      " 7. away: 106\n",
      " 8. one: 100\n",
      " 9. parking: 100\n",
      " 10. hijacking: 95\n",
      " 11. saw: 93\n",
      " 12. left: 89\n",
      " 13. noticed: 88\n",
      " 14. gone: 86\n",
      " 15. carjacking: 85\n",
      " 16. auto: 85\n",
      " 17. burglary: 84\n",
      " 18. door: 82\n",
      " 19. street: 77\n",
      " 20. stealing: 74\n",
      "\n",
      "Most common words for label 'Others':\n",
      " 1. someone: 107\n",
      " 2. noticed: 99\n",
      " 3. saw: 90\n",
      " 4. car: 81\n",
      " 5. one: 74\n",
      " 6. see: 73\n",
      " 7. reported: 66\n",
      " 8. illegal: 65\n",
      " 9. could: 62\n",
      " 10. quickly: 56\n",
      " 11. forest: 53\n",
      " 12. got: 53\n",
      " 13. area: 50\n",
      " 14. police: 47\n",
      " 15. called: 45\n",
      " 16. logging: 43\n",
      " 17. found: 42\n",
      " 18. fire: 42\n",
      " 19. man: 42\n",
      " 20. would: 41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def most_common_words(label, dataframe, n=30):\n",
    "    \"\"\"\n",
    "    Given a label and a dataframe, returns the n most common words excluding stop words.\n",
    "\n",
    "    Parameters:\n",
    "    - label (str): The label to filter by (e.g., 'Murder', 'Robbery').\n",
    "    - dataframe (pd.DataFrame): The dataframe containing the text data.\n",
    "    - n (int): The number of most common words to return.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples with the most common words and their frequencies.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe by the given label where the label is 1\n",
    "    label_data = dataframe[dataframe[label] == 1]\n",
    "\n",
    "    # Combine all text entries into one large string\n",
    "    all_text = ' '.join(label_data['Text'].tolist())\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    all_text = all_text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    all_text = all_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(all_text)\n",
    "\n",
    "    # Filter out stop words and words with length less than 2\n",
    "    filtered_words = [word for word in words if word not in STOP_WORDS and len(word) > 1]\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(filtered_words)\n",
    "\n",
    "    # Return the n most common words\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "for label in LABELS:\n",
    "    print(f\"\\nMost common words for label '{label}':\")\n",
    "    common_words = most_common_words(label, DATASET['train'], n=20)\n",
    "    \n",
    "    \n",
    "    for idx, (word, frequency) in enumerate(common_words, start=1):\n",
    "        print(f\" {idx}. {word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged datasets saved as new_train.csv, new_test.csv, and new_val.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your original and cleaned augmented CSV files\n",
    "dataset_dir = \"data/train_test_val/\"\n",
    "original_train_file = dataset_dir + \"train.csv\"\n",
    "original_test_file = dataset_dir + \"test.csv\"\n",
    "original_val_file = dataset_dir + \"val.csv\"\n",
    "\n",
    "cleaned_augmented_train_file = dataset_dir + \"cleaned_augmented_train.csv\"\n",
    "cleaned_augmented_test_file = dataset_dir + \"cleaned_augmented_test.csv\"\n",
    "cleaned_augmented_val_file = dataset_dir + \"cleaned_augmented_val.csv\"\n",
    "\n",
    "# Load the original and cleaned augmented CSV files\n",
    "original_train = pd.read_csv(original_train_file)\n",
    "original_test = pd.read_csv(original_test_file)\n",
    "original_val = pd.read_csv(original_val_file)\n",
    "\n",
    "cleaned_augmented_train = pd.read_csv(cleaned_augmented_train_file)\n",
    "cleaned_augmented_test = pd.read_csv(cleaned_augmented_test_file)\n",
    "cleaned_augmented_val = pd.read_csv(cleaned_augmented_val_file)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "new_train = pd.concat([original_train, cleaned_augmented_train], ignore_index=True)\n",
    "new_test = pd.concat([original_test, cleaned_augmented_test], ignore_index=True)\n",
    "new_val = pd.concat([original_val, cleaned_augmented_val], ignore_index=True)\n",
    "\n",
    "# Save the merged datasets to new CSV files\n",
    "# new_train.to_csv(dataset_dir + \"new_train.csv\", index=False)\n",
    "# new_test.to_csv(dataset_dir + \"new_test.csv\", index=False)\n",
    "new_val.to_csv(dataset_dir + \"new_val.csv\", index=False)\n",
    "\n",
    "print(\"Merged datasets saved as new_train.csv, new_test.csv, and new_val.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete and saved as preprocessed_train.csv, preprocessed_test.csv, and preprocessed_val.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = re.sub(r'[A-Z]', lambda y: y.group(0).lower(), text)\n",
    "    \n",
    "    # Remove unimportant links\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "    # Remove emojis (non-ASCII characters)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Remove usernames (mentions)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove punctuations and replace with space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Remove hashtags (keeping the text but removing the # symbol)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "dataset_dir = \"data/train_test_val/\"\n",
    "new_train_file = dataset_dir + \"new_train.csv\"\n",
    "new_test_file = dataset_dir + \"new_test.csv\"\n",
    "new_val_file = dataset_dir + \"new_val.csv\"\n",
    "\n",
    "new_train = pd.read_csv(new_train_file)\n",
    "new_test = pd.read_csv(new_test_file)\n",
    "new_val = pd.read_csv(new_val_file)\n",
    "\n",
    "new_train['Text'] = new_train['Text'].apply(preprocess)\n",
    "new_test['Text'] = new_test['Text'].apply(preprocess)\n",
    "new_val['Text'] = new_val['Text'].apply(preprocess)\n",
    "\n",
    "# new_train.to_csv(dataset_dir + \"preprocessed_train.csv\", index=False)\n",
    "# new_test.to_csv(dataset_dir + \"preprocessed_test.csv\", index=False)\n",
    "new_val.to_csv(dataset_dir + \"preprocessed_val.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing complete and saved as preprocessed_train.csv, preprocessed_test.csv, and preprocessed_val.csv!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
