{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.5/417.5 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from transformers[torch]) (2024.7.24)\n",
      "Requirement already satisfied: packaging>=20.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from transformers[torch]) (24.1)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from transformers[torch]) (4.66.5)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting accelerate>=0.21.0\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m324.4/324.4 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: jinja2 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.8-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.3/126.3 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.3/167.3 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, urllib3, sympy, safetensors, pyyaml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\n",
      "Successfully installed accelerate-0.34.2 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.0 fsspec-2024.9.0 huggingface-hub-0.24.7 idna-3.8 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 pyyaml-6.0.2 requests-2.32.3 safetensors-0.4.5 sympy-1.13.2 tokenizers-0.19.1 torch-2.4.1 transformers-4.44.2 triton-3.0.0 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: triton==3.0.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: jinja2 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Homicide</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Physical Injuries</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Theft</th>\n",
       "      <th>Carnapping</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1495</td>\n",
       "      <td>i was preparing to go to bed when i heard a lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>354</td>\n",
       "      <td>one evening i was watching television when i h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233</td>\n",
       "      <td>taking a shortcut home across a dark footbridg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>614</td>\n",
       "      <td>i was the designated driver that night so i wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3507</td>\n",
       "      <td>a cousin of mine was kidnapped as a baby by hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>1131</td>\n",
       "      <td>late one night i was watching tv when i heard ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>1295</td>\n",
       "      <td>while lounging by the pool i saw a man pacing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>861</td>\n",
       "      <td>the couple next door had always been volatile ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>3508</td>\n",
       "      <td>a friend of mine was kidnapped while doing his...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>3175</td>\n",
       "      <td>i noticed an unmarked delivery van in our offi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               Text  Murder  \\\n",
       "0     1495  i was preparing to go to bed when i heard a lo...       0   \n",
       "1      354  one evening i was watching television when i h...       1   \n",
       "2     1233  taking a shortcut home across a dark footbridg...       0   \n",
       "3      614  i was the designated driver that night so i wa...       0   \n",
       "4     3507  a cousin of mine was kidnapped as a baby by hi...       0   \n",
       "...    ...                                                ...     ...   \n",
       "2395  1131  late one night i was watching tv when i heard ...       0   \n",
       "2396  1295  while lounging by the pool i saw a man pacing ...       0   \n",
       "2397   861  the couple next door had always been volatile ...       0   \n",
       "2398  3508  a friend of mine was kidnapped while doing his...       0   \n",
       "2399  3175  i noticed an unmarked delivery van in our offi...       0   \n",
       "\n",
       "      Homicide  Robbery  Physical Injuries  Rape  Theft  Carnapping  Others  \n",
       "0            0        1                  1     0      0           0       0  \n",
       "1            0        0                  0     0      0           0       0  \n",
       "2            0        1                  0     0      0           0       0  \n",
       "3            1        0                  0     0      0           0       0  \n",
       "4            0        0                  0     0      0           0       1  \n",
       "...        ...      ...                ...   ...    ...         ...     ...  \n",
       "2395         0        1                  0     0      0           0       0  \n",
       "2396         0        1                  0     0      0           0       0  \n",
       "2397         1        0                  0     0      0           0       0  \n",
       "2398         0        0                  0     0      0           0       1  \n",
       "2399         0        0                  0     0      0           1       0  \n",
       "\n",
       "[2400 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"data/train_test_val\"\n",
    "\n",
    "dataset = {\n",
    "    'train': pd.read_csv(dataset_dir + '/train.csv').reset_index(drop=True),  # \"\"\" encoding='cp1252' \"\"\" insert between train_data.csv and .reset index as parameter\n",
    "    'test': pd.read_csv(dataset_dir + '/test.csv').reset_index(drop=True),  # \"\"\" encoding='cp1252' \"\"\" insert between train_data.csv and .reset index as parameter\n",
    "    'val': pd.read_csv(dataset_dir + '/val.csv').reset_index(drop=True),  # \"\"\" encoding='cp1252' \"\"\" insert between train_data.csv and .reset index as parameter\n",
    "}\n",
    "\n",
    "dataset['val']\n",
    "dataset['test']\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Murder',\n",
       " 'Homicide',\n",
       " 'Robbery',\n",
       " 'Physical Injuries',\n",
       " 'Rape',\n",
       " 'Theft',\n",
       " 'Carnapping',\n",
       " 'Others']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = [label for label in dataset['train'].keys() if label not in ['ID', 'Text']]\n",
    "id2label = {idx:label for idx, label in enumerate(LABELS)}\n",
    "label2id = {label:idx for idx, label in enumerate(LABELS)}\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, labels, max_token_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "        self.encoded_dataset = self.encode_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_dataset[index]\n",
    "    \n",
    "\n",
    "    def encode_dataset(self):\n",
    "        encoded_dataset = []\n",
    "        for index, data in tqdm(self.data.iterrows()):\n",
    "            encoded_data = self.encode_data(data)\n",
    "            encoded_dataset.append(encoded_data)\n",
    "        return encoded_dataset\n",
    "\n",
    "    def encode_data(self, data):\n",
    "        text = data[\"Text\"]\n",
    "\n",
    "        #preprocess text\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        labels = [data[label] for label in self.labels]\n",
    "\n",
    "        representation = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }\n",
    "        return representation\n",
    "    \n",
    "\n",
    "\n",
    "class CrimeDataLoader:\n",
    "    def __init__(self, dataset, labels, tokenizer, batch_size=8):\n",
    "        self.train_dataset = CrimeDataset(dataset['train'], tokenizer, labels)\n",
    "        self.val_dataset = CrimeDataset(dataset['val'], tokenizer, labels)\n",
    "        self.test_dataset = CrimeDataset(dataset['test'], tokenizer, labels)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [00:01, 2086.17it/s]\n",
      "400it [00:00, 2097.94it/s]\n",
      "1200it [00:00, 2107.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader = CrimeDataLoader(dataset=dataset, tokenizer=tokenizer, labels=LABELS)\n",
    "\n",
    "def split_dataloader(dataloader):\n",
    "    train_dataloader = dataloader.train_dataloader()\n",
    "    val_dataloader = dataloader.val_dataloader()\n",
    "    test_dataloader = dataloader.test_dataloader()\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = split_dataloader(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [00:01, 2033.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Decoded sentence\n",
      "[CLS] taking a shortcut home across a dark footbridge a man suddenly blocked my path he pulled out a knife and demanded my wallet using menacing threats i was scared and handed it over without hesitation [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Label IDs of example 1\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "\n",
      "IDs to Labels\n",
      "['Robbery']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "example = CrimeDataset(dataset['train'], tokenizer, LABELS).__getitem__(2)\n",
    "print(example.keys())\n",
    "print()\n",
    "print(\"Decoded sentence\")\n",
    "print(tokenizer.decode(example['input_ids']))\n",
    "print()\n",
    "print(\"Label IDs of example 1\")\n",
    "print(example['labels'])\n",
    "print()\n",
    "print(\"IDs to Labels\")\n",
    "print([id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syke/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [00:01, 1905.14it/s]\n",
      "1200it [00:00, 2000.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CrimeDataset(dataset['train'], tokenizer, LABELS)\n",
    "test_dataset = CrimeDataset(dataset['test'], tokenizer, LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(LABELS),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/syke/Colorful/Github/xlnet/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"models/bert-classifier-wrapper\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/syke/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.6.0\n",
      "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.14.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [00:01, 2082.01it/s]\n",
      "1200it [00:00, 2182.61it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=CrimeDataset(dataset['train'], tokenizer, LABELS),\n",
    "    eval_dataset=CrimeDataset(dataset['test'], tokenizer, LABELS),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 300/1500 [03:27<11:42,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11899271607398987, 'eval_f1': 0.9621740727139185, 'eval_roc_auc': 0.9651633291407689, 'eval_accuracy': 0.9166666666666666, 'eval_runtime': 31.4072, 'eval_samples_per_second': 38.208, 'eval_steps_per_second': 4.776, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 500/1500 [05:26<09:46,  1.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2095, 'grad_norm': 0.4453684091567993, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 600/1500 [06:56<08:45,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05384119600057602, 'eval_f1': 0.9841954022988506, 'eval_roc_auc': 0.9864242630546592, 'eval_accuracy': 0.9683333333333334, 'eval_runtime': 31.4065, 'eval_samples_per_second': 38.209, 'eval_steps_per_second': 4.776, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 900/1500 [10:27<05:51,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.037070807069540024, 'eval_f1': 0.9824686940966011, 'eval_roc_auc': 0.9870021394830524, 'eval_accuracy': 0.97, 'eval_runtime': 31.4505, 'eval_samples_per_second': 38.155, 'eval_steps_per_second': 4.769, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1000/1500 [11:28<04:53,  1.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0467, 'grad_norm': 0.12222081422805786, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1200/1500 [13:57<02:55,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03159654140472412, 'eval_f1': 0.9824057450628366, 'eval_roc_auc': 0.9855304478809073, 'eval_accuracy': 0.9666666666666667, 'eval_runtime': 31.4643, 'eval_samples_per_second': 38.138, 'eval_steps_per_second': 4.767, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [16:56<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0306, 'grad_norm': 0.1690213978290558, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [17:30<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.030043931677937508, 'eval_f1': 0.9795918367346939, 'eval_roc_auc': 0.985042226228277, 'eval_accuracy': 0.9666666666666667, 'eval_runtime': 31.3409, 'eval_samples_per_second': 38.289, 'eval_steps_per_second': 4.786, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [17:36<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1056.8096, 'train_samples_per_second': 11.355, 'train_steps_per_second': 1.419, 'train_loss': 0.09562087059020996, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.09562087059020996, metrics={'train_runtime': 1056.8096, 'train_samples_per_second': 11.355, 'train_steps_per_second': 1.419, 'total_flos': 789375688704000.0, 'train_loss': 0.09562087059020996, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:30<00:00,  4.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05384119600057602,\n",
       " 'eval_f1': 0.9841954022988506,\n",
       " 'eval_roc_auc': 0.9864242630546592,\n",
       " 'eval_accuracy': 0.9683333333333334,\n",
       " 'eval_runtime': 30.9894,\n",
       " 'eval_samples_per_second': 38.723,\n",
       " 'eval_steps_per_second': 4.84,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Parang gago tong mga pari na to\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = outputs.logits\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(output.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the best model\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(args.output_dir + \"/checkpoint-1500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    TP = confusion_matrix[1, 1]  # True Positives\n",
    "    FP = confusion_matrix[0, 1]  # False Positives0..\n",
    "    FN = confusion_matrix[1, 0]  # False Negatives\n",
    "    TN = confusion_matrix[0, 0]  # True Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    }\n",
    "\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    xor_result = np.logical_xor(y_true, y_pred)\n",
    "    xor_sum = np.sum(xor_result)\n",
    "    hamming_loss = xor_sum / (y_true.shape[0] * y_true.shape[1])\n",
    "    return hamming_loss\n",
    "\n",
    "def multilabel_metrics(predictions, labels, mode=\"validation\", threshold=0.5):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probabilities = sigmoid(torch.Tensor(predictions))\n",
    "\n",
    "    y_pred = np.zeros(probabilities.shape)\n",
    "    y_pred[np.where(probabilities >= threshold)] = 1\n",
    "\n",
    "    y_true = np.zeros(labels.shape)\n",
    "    y_true[np.where(labels == 1)] = 1\n",
    "\n",
    "    confusion_matrix = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    label_metrics = {}\n",
    "    classes = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        metrics = calculate_metrics(confusion_matrix[i])\n",
    "        label_metrics[class_name] = metrics\n",
    "\n",
    "        precision = metrics['Precision'] * 100\n",
    "        recall = metrics['Recall'] * 100\n",
    "        f1 = metrics['F1-Score'] * 100\n",
    "\n",
    "        print(f\"{class_name}\")\n",
    "        print(f\"    Precision: {precision:.2f}%\")\n",
    "        print(f\"    Recall: {recall:.2f}%\")\n",
    "        print(f\"    F-Measure: {f1:.2f}%\")\n",
    "\n",
    "    label_metrics['hamming_loss'] = hamming_loss(y_true, y_pred)\n",
    "    print(f\"\\nHamming Loss ({mode}): {label_metrics['hamming_loss']}\")\n",
    "\n",
    "    return label_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_set\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# save dataframe to csv for experiment 1\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m df_set \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 34\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_dataset, labels)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_id, data \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     33\u001b[0m         input_sentence \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m         encoded_data \u001b[38;5;241m=\u001b[39m \u001b[43mencode_data\u001b[49m(data)\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# get true labels\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         true_labels \u001b[38;5;241m=\u001b[39m encoded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_data' is not defined"
     ]
    }
   ],
   "source": [
    "def get_cm_eval(y_pred, y_true): # TN, FP, FN, TP\n",
    "\n",
    "    cm_eval = []\n",
    "\n",
    "    for idx, label in enumerate(y_pred):\n",
    "         \n",
    "        if   y_pred[idx] == 0 and y_true[idx] == 0:\n",
    "            cm_eval.append(\"TN\") \n",
    "        elif y_pred[idx] == 1 and y_true[idx] == 0:\n",
    "            cm_eval.append(\"FP\") \n",
    "        elif y_pred[idx] == 0 and y_true[idx] == 1:\n",
    "            cm_eval.append(\"FN\") \n",
    "        elif y_pred[idx] == 1 and y_true[idx] == 1:\n",
    "            cm_eval.append(\"TP\") \n",
    "\n",
    "    return cm_eval\n",
    "    \n",
    "\n",
    "def test_model(model, test_dataset, labels=LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "\n",
    "    # initialize dataframe for each labels\n",
    "    header_row = ['ID', 'Text', 'Actual Labels', 'Predicted Labels', 'Evaluation']\n",
    "    df_set = {}\n",
    "    for label in labels:\n",
    "        df_set[label] = pd.DataFrame(columns=header_row) \n",
    "\n",
    "    # get rows\n",
    "    for input_id, data in test_dataset.iterrows():\n",
    "\n",
    "            input_sentence = data['Text']\n",
    "            encoded_data = encode_data(data)\n",
    "\n",
    "            # get true labels\n",
    "            true_labels = encoded_data['labels']\n",
    "            y_true = np.zeros(true_labels.shape)\n",
    "            y_true[np.where(true_labels >= threshold)] = 1\n",
    "\n",
    "            # get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=encoded_data['input_ids'], attention_mask=encoded_data['attention_mask'])\n",
    "\n",
    "            logits = outputs.logits\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            probabilities = sigmoid(logits.squeeze().cpu())\n",
    "            y_pred = np.zeros(probabilities.shape)\n",
    "            y_pred[np.where(probabilities >= threshold)] = 1\n",
    "\n",
    "             # get evaluation\n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                df_row = [input_id, input_sentence]\n",
    "\n",
    "                if   y_pred[idx] == 0 and y_true[idx] == 0:\n",
    "                    y_eval = \"TN\" \n",
    "                elif y_pred[idx] == 1 and y_true[idx] == 0:\n",
    "                    y_eval = \"FP\" \n",
    "                elif y_pred[idx] == 0 and y_true[idx] == 1:\n",
    "                    y_eval = \"FN\" \n",
    "                elif y_pred[idx] == 1 and y_true[idx] == 1:\n",
    "                    y_eval = \"TP\"  \n",
    "\n",
    "                df_row.append(y_true[idx].astype(int))\n",
    "                df_row.append(y_pred[idx].astype(int))\n",
    "                df_row.append(y_eval)\n",
    "\n",
    "                print(df_row)\n",
    "                df_set[label].loc[len(df_set[label])] = df_row\n",
    "\n",
    "            print()\n",
    "\n",
    "    return df_set\n",
    "\n",
    "# save dataframe to csv for experiment 1\n",
    "df_set = test_model(trainer.model, dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>True Positives (TP)</th>\n",
       "      <th>True Negatives (TN)</th>\n",
       "      <th>False Positives (FP)</th>\n",
       "      <th>False Negatives (FN)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>123</td>\n",
       "      <td>489</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gender</td>\n",
       "      <td>118</td>\n",
       "      <td>481</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.883895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Physical</td>\n",
       "      <td>112</td>\n",
       "      <td>465</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.808664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Race</td>\n",
       "      <td>93</td>\n",
       "      <td>507</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.853211</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Religion</td>\n",
       "      <td>112</td>\n",
       "      <td>511</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Others</td>\n",
       "      <td>73</td>\n",
       "      <td>519</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.793478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  True Positives (TP)  True Negatives (TN)  False Positives (FP)  \\\n",
       "0       Age                  123                  489                    11   \n",
       "1    Gender                  118                  481                    15   \n",
       "2  Physical                  112                  465                    17   \n",
       "3      Race                   93                  507                    14   \n",
       "4  Religion                  112                  511                     5   \n",
       "5    Others                   73                  519                    15   \n",
       "\n",
       "   False Negatives (FN)  Precision    Recall  F-measure  \n",
       "0                     7   0.917910  0.946154   0.931818  \n",
       "1                    16   0.887218  0.880597   0.883895  \n",
       "2                    36   0.868217  0.756757   0.808664  \n",
       "3                    16   0.869159  0.853211   0.861111  \n",
       "4                     2   0.957265  0.982456   0.969697  \n",
       "5                    23   0.829545  0.760417   0.793478  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_metrics(eval_count: dict):\n",
    "    TP = eval_count['TP'] # True Positives\n",
    "    FP = eval_count['FP']  # False Positives\n",
    "    FN = eval_count['FN']  # False Negatives\n",
    "    TN = eval_count['TN']  # True Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Experiment Paper 2\n",
    "def exp2(df_set, labels=LABELS):\n",
    "    \n",
    "    header_row = ['Label', 'True Positives (TP)', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'Precision', 'Recall', 'F-measure']\n",
    "    exp2_df = pd.DataFrame(columns=header_row)\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "\n",
    "        # evaluation count for each label\n",
    "        eval_count = {\n",
    "            'TP' : 0,\n",
    "            'TN' : 0,\n",
    "            'FP' : 0,\n",
    "            'FN' : 0\n",
    "        }\n",
    "\n",
    "        label_eval_column = df_set[label]['Evaluation']\n",
    "\n",
    "        for eval in label_eval_column:\n",
    "            eval_count[eval] += 1\n",
    "        \n",
    "        precision, recall, f1_score = get_metrics(eval_count) \n",
    "\n",
    "        exp2_df.loc[len(exp2_df)] = [\n",
    "            label, \n",
    "            eval_count['TP'],\n",
    "            eval_count['TN'],\n",
    "            eval_count['FP'],\n",
    "            eval_count['FN'],\n",
    "            precision,\n",
    "            recall,\n",
    "            f1_score\n",
    "        ]\n",
    "    \n",
    "    return exp2_df\n",
    "\n",
    "exp2_df = exp2(df_set, labels=LABELS)\n",
    "exp2_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
